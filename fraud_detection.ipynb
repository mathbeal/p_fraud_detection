{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying multiples naive approach on fraud_detection.\n",
    "\n",
    "# Sources:\n",
    "# - https://www.data-blogger.com/2017/06/15/fraud-detection-a-simple-machine-learning-approach/\n",
    "# - https://gist.github.com/abnvanand/0aba78f2ce94c5aa9d6a1473a0ed8df7\n",
    "# - https://www.datasciencesmachinelearning.com/2018/11/confusion-matrix-accuracy-precision.html\n",
    "# - https://www.kaggle.com/tboyle10/methods-for-dealing-with-imbalanced-data\n",
    "\n",
    "FILES_DIR = \"ieee-fraud-detection\"\n",
    "# DataSet: Unzip data in ieee-fraud-detection\n",
    "# - https://www.kaggle.com/c/ieee-fraud-detection/data\n",
    "\n",
    "import os\n",
    "assert os.path.exists(FILES_DIR), f'missing folder {FILES_DIR}, download data from Kaggle'\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 75)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=11):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_file = lambda fname: FILES_DIR + '/' + fname\n",
    "\n",
    "def merge_csv_as_df(transaction, identity, on='TransactionID'):\n",
    "    transaction = pd.read_csv(get_file(f'{transaction}.csv'))\n",
    "    identity = pd.read_csv(get_file(f'{identity}.csv'))\n",
    "    print(\"from\", transaction.shape)\n",
    "    print(\"and from\", identity.shape)\n",
    "    X = transaction.merge(identity, on=on, how='left')\n",
    "    del transaction\n",
    "    del identity\n",
    "    print(\"to\", X.shape)\n",
    "    return X\n",
    "\n",
    "X_train = merge_csv_as_df('train_transaction', 'train_identity')\n",
    "y_train = X_train['isFraud']\n",
    "print(X_train['isFraud'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns \n",
    "infos = {}\n",
    "for c in X_train.columns:\n",
    "    infos[c] = len(X_train[c].unique()) / len(X_train[c]) * 100.\n",
    "features = [ c for c, score in infos.items() if 1 <= score <= 85. ]\n",
    "print(f\"{len(features)} are selected.\")\n",
    "\n",
    "# Display dtype of features\n",
    "from collections import defaultdict\n",
    "dtypes = defaultdict(int)\n",
    "for feature in features:\n",
    "    dtypes[X_train[feature].dtype] += 1\n",
    "print(dtypes)\n",
    "    \n",
    "# Reduce dataframe to selected features\n",
    "X_train = X_train[features]\n",
    "origin_X_train = X_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.fillna(-999, inplace=True)\n",
    "\n",
    "def normalize(X):\n",
    "    \"\"\"\n",
    "    Make the distribution of the values of each variable similar by subtracting the mean and by dividing by the standard deviation.\n",
    "    \"\"\"\n",
    "    for feature in X.columns:\n",
    "        X[feature] -= X[feature].mean()\n",
    "        X[feature] /= X[feature].std()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "X = X_train.copy()\n",
    "y = y_train.copy()\n",
    "\n",
    "# Define the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Define the splitter for splitting the data in a train set and a test set\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=0)\n",
    "\n",
    "# Loop through the splits (only one)\n",
    "for train_indices, test_indices in splitter.split(X, y):\n",
    "    # Select the train and test data\n",
    "    _X_train, _y_train = X.iloc[train_indices], y.iloc[train_indices]\n",
    "    _X_test, _y_test = X.iloc[test_indices], y.iloc[test_indices]\n",
    "    \n",
    "    # Normalize the data\n",
    "    _X_train = normalize(_X_train)\n",
    "    _X_test = normalize(_X_test)\n",
    "    \n",
    "    # Fit and predict!\n",
    "    model.fit(_X_train, _y_train)\n",
    "    _y_pred = model.predict(_X_test)\n",
    "    \n",
    "    # And finally: show the results\n",
    "    print(classification_report(_y_test, _y_pred))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(_y_test, _y_pred)\n",
    "x = print_confusion_matrix(cm, class_names=['!Fraud', 'Fraud'])\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(_y_test, _y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try 2: Logistic Regression on PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def do_pca(X, y_train, n_components=4):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    principalComponents = pca.fit_transform(X)\n",
    "    principalDf = pd.DataFrame(data = principalComponents,\n",
    "                               columns = [f'pc_{i}' for i in range(n_components)])\n",
    "    tp = pd.concat([principalDf, y_train], axis=1)\n",
    "    return tp, pca\n",
    "\n",
    "n_components = 5\n",
    "\n",
    "finalDf, pca = do_pca(X_train.copy(), y.copy(), n_components=n_components)\n",
    "print(\"#\"*80)\n",
    "print(f\"{n_components} components explain {100.*pca.explained_variance_ratio_.sum():.2f}% of the information.\")\n",
    "print(\"#\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display PairPlot PCA\n",
    "print(finalDf['isFraud'].unique())\n",
    "is_fraud = {0.0:'!Fraud', 1.0:'Fraud'}\n",
    "finalDf['isFraud'] = finalDf['isFraud'].map(is_fraud)\n",
    "print(finalDf['isFraud'].unique())\n",
    "sns.pairplot(finalDf, hue='isFraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression on PCA\n",
    "X = X_train.copy()\n",
    "y = y_train.copy()\n",
    "\n",
    "# Define the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Define the splitter for splitting the data in a train set and a test set\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=0)\n",
    "\n",
    "# Loop through the splits (only one)\n",
    "for train_indices, test_indices in splitter.split(X, y):\n",
    "    # Select the train and test data\n",
    "    _X_train, _y_train = X.iloc[train_indices], y.iloc[train_indices]\n",
    "    _X_test, _y_test = X.iloc[test_indices], y.iloc[test_indices]\n",
    "    \n",
    "    # Normalize the data\n",
    "    _X_train = normalize(_X_train)\n",
    "    _X_test = normalize(_X_test)\n",
    "    \n",
    "    # Fit and predict!\n",
    "    model.fit(_X_train, _y_train)\n",
    "    _y_pred = model.predict(_X_test)\n",
    "    \n",
    "    # And finally: show the results\n",
    "    print(classification_report(_y_test, _y_pred))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(_y_test, _y_pred)\n",
    "x = print_confusion_matrix(cm, class_names=['!Fraud', 'Fraud'])\n",
    "plt.show()\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(_y_test, _y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try 3: RandomForestClassifier on PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = X_train.copy()\n",
    "y = y_train.copy()\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestClassifier(n_estimators = 100, n_jobs = 1, \n",
    "                             random_state = 2016,\n",
    "                             class_weight='balanced',oob_score=True)\n",
    "\n",
    "# Define the splitter for splitting the data in a train set and a test set\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=0)\n",
    "\n",
    "# Loop through the splits (only one)\n",
    "for train_indices, test_indices in splitter.split(X, y):\n",
    "    # Select the train and test data\n",
    "    _X_train, _y_train = X.iloc[train_indices], y.iloc[train_indices]\n",
    "    _X_test, _y_test = X.iloc[test_indices], y.iloc[test_indices]\n",
    "    \n",
    "    # Normalize the data\n",
    "    _X_train = normalize(_X_train)\n",
    "    _X_test = normalize(_X_test)\n",
    "    \n",
    "    # Fit and predict!\n",
    "    model.fit(_X_train, _y_train)\n",
    "    _y_pred = model.predict(_X_test)\n",
    "    \n",
    "    # And finally: show the results\n",
    "    print(classification_report(_y_test, _y_pred))\n",
    "    \n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(_y_test, _y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(_y_test, _y_pred)\n",
    "x = print_confusion_matrix(cm, class_names=['!Fraud', 'Fraud'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try 4: Deal with imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying code from https://www.kaggle.com/tboyle10/methods-for-dealing-with-imbalanced-data\n",
    "# to check any improvements.\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate input features and target\n",
    "y = y_train.copy()\n",
    "X = X_train.copy()\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n",
    "\n",
    "sm = SMOTE(random_state=27, ratio=1.0)\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "smote = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    "\n",
    "smote_pred = smote.predict(X_test)\n",
    "cm = confusion_matrix(y_test, smote_pred)\n",
    "x = print_confusion_matrix(cm, class_names=['!Fraud', 'Fraud'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking accuracy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, smote_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
